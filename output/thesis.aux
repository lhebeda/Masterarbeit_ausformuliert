\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@input{vorspann.aux}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\citation{lecun2015deep}
\citation{krizhevsky2012imagenet}
\citation{he2017mask}
\citation{zollhoefer2018state}
\citation{sun2017revisiting}
\citation{everingham2010pascal}
\citation{lin2014microsoft}
\citation{mildenhall2021nerf}
\citation{barron2021mip}
\citation{kerbl3Dgaussians}
\citation{fridovich2023k}
\citation{CutAndSplat2024}
\citation{tobin2017domain}
\citation{denninger2019blenderproc}
\citation{dwibedi2017cutpaste}
\citation{zanjani2025gaussian}
\citation{godard2017unsupervised}
\citation{bertasius2019maskprop}
\citation{luiten2024dynamic}
\citation{yang2024deformable}
\citation{CutAndSplat2024}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Introduction}{1}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}Motivation}{1}{subsection.0.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}Structure of the Work}{2}{subsection.0.1.2}\protected@file@percent }
\citation{mildenhall2021nerf}
\citation{mildenhall2021nerf}
\citation{mildenhall2021nerf}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Fundamentals}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:Fundamentals}{{1}{3}{Fundamentals}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Neural Radiance Fields}{3}{section.1.1}\protected@file@percent }
\newlabel{sec:Fundamentals_NeRF}{{1.1}{3}{Neural Radiance Fields}{section.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Core principle of NeRF: A model is optimized from input images and can subsequently generate novel views of the scene (after \cite  {mildenhall2021nerf}).}}{3}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nerf_teaser}{{1.1}{3}{Core principle of NeRF: A model is optimized from input images and can subsequently generate novel views of the scene (after \cite {mildenhall2021nerf})}{figure.caption.3}{}}
\citation{mildenhall2021nerf}
\citation{mildenhall2021nerf}
\citation{kerbl3Dgaussians}
\citation{kerbl3Dgaussians}
\citation{kerbl3Dgaussians}
\citation{schoenberger2016mvs}
\citation{schoenberger2016sfm}
\citation{kerbl3Dgaussians}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces NeRF pipeline: The MLP maps 5D inputs to color and density, which are composed into an image via volumetric rendering (after \cite  {mildenhall2021nerf}).}}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:nerf_pipeline}{{1.2}{4}{NeRF pipeline: The MLP maps 5D inputs to color and density, which are composed into an image via volumetric rendering (after \cite {mildenhall2021nerf})}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Gaussian Splatting}{4}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Scene Representation with 3D Gaussians}{4}{subsection.1.2.1}\protected@file@percent }
\citation{zwicker2001ewa}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Optimization and rendering pipeline of Gaussian Splatting: The process begins with a sparse structure-from-motion (SfM) point cloud, which serves as the basis for creating an initial set of 3D Gaussian functions. These Gaussians are refined through iterative optimization, with their density adaptively controlled to ensure accurate scene representation. A fast tile-based rasterizer enables competitive rendering times compared to modern radiance field methods (figure from \cite  {kerbl3Dgaussians}).}}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:overview}{{1.3}{5}{Optimization and rendering pipeline of Gaussian Splatting: The process begins with a sparse structure-from-motion (SfM) point cloud, which serves as the basis for creating an initial set of 3D Gaussian functions. These Gaussians are refined through iterative optimization, with their density adaptively controlled to ensure accurate scene representation. A fast tile-based rasterizer enables competitive rendering times compared to modern radiance field methods (figure from \cite {kerbl3Dgaussians})}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Rendering with 3D Gaussian Splatting}{5}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Projection of 3D Gaussians}{6}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Simplified Covariance Computation}{6}{section*.7}\protected@file@percent }
\newlabel{eq:calc_sigma}{{1.3}{6}{Simplified Covariance Computation}{equation.1.3}{}}
\citation{kerbl3Dgaussians}
\citation{kerbl3Dgaussians}
\citation{kerbl3Dgaussians}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Optimization}{7}{subsection.1.2.3}\protected@file@percent }
\newlabel{eq:loss_GS}{{1.4}{7}{Optimization}{equation.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adaptive Control of 3D Gaussians}{7}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Differentiable Rasterizer}{7}{subsection.1.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Adaptive density control: The top row shows under-reconstruction, where small geometries (black outlines) are supplemented by cloning a Gaussian. The bottom row shows over-reconstruction, where a large Gaussian is split into two smaller ones (after \cite  {kerbl3Dgaussians}).}}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:adaptivecontrol}{{1.4}{8}{Adaptive density control: The top row shows under-reconstruction, where small geometries (black outlines) are supplemented by cloning a Gaussian. The bottom row shows over-reconstruction, where a large Gaussian is split into two smaller ones (after \cite {kerbl3Dgaussians})}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Tile-based Rasterization Process}{8}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Rendering and Blending}{8}{section*.11}\protected@file@percent }
\newlabel{eq:projection_3D_gaussians}{{1.6}{9}{Rendering and Blending}{equation.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Backward Pass and Gradient Computation}{9}{section*.12}\protected@file@percent }
\citation{mildenhall2021nerf}
\citation{barron2021mip}
\citation{barron2023zipnerf}
\citation{kerbl3Dgaussians}
\citation{gaussian_grouping}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}State of the Art}{10}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Gaussian Splatting for Scene Representation}{10}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Dynamic Extensions of Gaussian Splatting}{10}{section.2.2}\protected@file@percent }
\citation{luiten2024dynamic}
\citation{stearnsmarbels}
\citation{yang2024deformable}
\citation{pumarola2021d}
\citation{yang2023gs4d}
\citation{lispacetimegaussianfeaturesplattingrealtime2024}
\citation{wu20244d}
\citation{fridovich2023k}
\citation{luiten2024dynamic}
\citation{yang2023gs4d}
\citation{Dwibedi2017}
\citation{Tobin2017}
\citation{Liu2018}
\citation{Li2023MattingSurvey}
\citation{Denninger2019}
\citation{Lee2018}
\citation{Kirillov2023}
\citation{Bertasius2020}
\citation{Godard2019}
\citation{Niu2021}
\citation{InpaintingLimitations2019}
\citation{MonoDepthLimitations2018}
\citation{Kirillov2023}
\citation{MaskPropagation2019}
\citation{He2017MaskRCNN}
\citation{Detectron22020}
\citation{MaskPropagation2019}
\citation{zanjani2025gaussiansplattingeffectivedata}
\citation{SyntheticDrone2023}
\citation{SurgicalGS2023}
\citation{MobileRobotsGS2024}
\citation{CutAndSplat2024}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Synthetic Compositing and Dataset Pipelines}{12}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Gaussian Splatting for Synthetic Dataset Generation}{12}{section.2.4}\protected@file@percent }
\citation{luiten2024dynamic}
\citation{yang2023gs4d}
\citation{yang2023gs4d}
\citation{yang2023gs4d}
\citation{yang2023gs4d}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Detailed Review of Prominent Dynamic GS Methods}{13}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting}{13}{subsection.2.5.1}\protected@file@percent }
\newlabel{sec:Real-Time4dgs}{{2.5.1}{13}{Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting}{subsection.2.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Extension of the Rendering Equation}{13}{section*.14}\protected@file@percent }
\citation{kerbl3Dgaussians}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The rendering pipeline of the proposed 4DGS method. For a given time \(t\) and view \(I\), each 4D Gaussian is decomposed into a conditional 3D Gaussian and a marginal 1D Gaussian. The conditional 3D Gaussian is then projected onto a 2D splat. Finally, the planar conditional Gaussian, the 1D marginal Gaussian, and the temporally varying, view-dependent color are combined to render the view \(I\) (figure adapted from \cite  {yang2023gs4d}).}}{14}{figure.caption.13}\protected@file@percent }
\newlabel{fig:pipeline_real_time_4D}{{2.1}{14}{The rendering pipeline of the proposed 4DGS method. For a given time \(t\) and view \(I\), each 4D Gaussian is decomposed into a conditional 3D Gaussian and a marginal 1D Gaussian. The conditional 3D Gaussian is then projected onto a 2D splat. Finally, the planar conditional Gaussian, the 1D marginal Gaussian, and the temporally varying, view-dependent color are combined to render the view \(I\) (figure adapted from \cite {yang2023gs4d})}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Scene Representation with 4D Gaussians}{14}{section*.15}\protected@file@percent }
\newlabel{eq:rotation_matrix}{{2.3}{14}{Scene Representation with 4D Gaussians}{equation.2.3}{}}
\citation{luiten2024dynamic}
\@writefile{toc}{\contentsline {subsubsection}{Projection onto the Image Plane}{15}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Spherical Harmonics for Color Representation}{15}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis}{15}{section.2.6}\protected@file@percent }
\citation{luiten2024dynamic}
\citation{luiten2024dynamic}
\@writefile{toc}{\contentsline {subsubsection}{Scene Representation}{16}{section*.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Schematic illustration of the local rotation similarity loss in D3DGS~\cite  {luiten2024dynamic}.}}{16}{figure.caption.19}\protected@file@percent }
\newlabel{fig:loss_fig_luiten}{{2.2}{16}{Schematic illustration of the local rotation similarity loss in D3DGS~\cite {luiten2024dynamic}}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{Physical Regularization}{16}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Local Rigidity Loss}{16}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Local Rotation Similarity Loss}{17}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Isometry Loss}{17}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimization}{17}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{18}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Model Training}{18}{section.3.1}\protected@file@percent }
\newlabel{sec:modeltraining}{{3.1}{18}{Model Training}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Data Preparation}{18}{subsection.3.1.1}\protected@file@percent }
\newlabel{sec:data_preparation}{{3.1.1}{18}{Data Preparation}{subsection.3.1.1}{}}
\citation{kerbl3Dgaussians}
\@writefile{toc}{\contentsline {subsubsection}{OpenCV Calibration}{19}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}3D Gaussian Splatting}{19}{subsection.3.1.2}\protected@file@percent }
\citation{yang20244dgs}
\citation{yuan20251000fps4dgaussian}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}4D Gaussian Splatting}{20}{subsection.3.1.3}\protected@file@percent }
\newlabel{sec:4dgs}{{3.1.3}{20}{4D Gaussian Splatting}{subsection.3.1.3}{}}
\citation{luiten2024dynamic}
\citation{luiten2024dynamic}
\citation{ravi2024sam2}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Dynamic 3D Gaussian Splatting}{21}{subsection.3.1.4}\protected@file@percent }
\newlabel{sec:method_dynamic3d}{{3.1.4}{21}{Dynamic 3D Gaussian Splatting}{subsection.3.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Dataset Preparation and Mask Generation}{21}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training Adjustments}{22}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Composition Pipeline}{22}{section.3.2}\protected@file@percent }
\newlabel{sec:compositionpipeline}{{3.2}{22}{Composition Pipeline}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  \textbf  {Overview of the proposed composition pipeline.} \textbf  {Pipeline A} converts multi-view captures into per-object 3DGS and D3DGS models via calibration, mask generation, and model training. \textbf  {Pipeline B} composes exported models into multi-object scenes, producing synchronized RGB, depth, segmentation, and occlusion annotations. }}{23}{figure.caption.28}\protected@file@percent }
\newlabel{fig:Ablauf}{{3.1}{23}{\textbf {Overview of the proposed composition pipeline.} \textbf {Pipeline A} converts multi-view captures into per-object 3DGS and D3DGS models via calibration, mask generation, and model training. \textbf {Pipeline B} composes exported models into multi-object scenes, producing synchronized RGB, depth, segmentation, and occlusion annotations}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Pipeline A: Reconstruction of 3D and Dynamic 3D Models}{23}{section.3.3}\protected@file@percent }
\citation{luiten2024dynamic}
\citation{ravi2024sam2}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Capture setup and calibration}{24}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Data Preparation: Mask Generation and Prompt Propagation}{24}{subsection.3.3.2}\protected@file@percent }
\newlabel{sec:maskgen}{{3.3.2}{24}{Data Preparation: Mask Generation and Prompt Propagation}{subsection.3.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Pipeline B: Composition and Synthetic Dataset Generation}{25}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Scene configuration and class indexing}{25}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Camera definition and interpolation}{25}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Placement duplication and motion handling}{26}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Mask rendering and mask types}{26}{subsection.3.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Occlusion scoring}{26}{section*.29}\protected@file@percent }
\citation{markley2007averaging}
\citation{Detectron22020}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}6D pose estimation}{27}{subsection.3.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.6}Keypoint detection and propagation}{28}{subsection.3.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.7}Rendered outputs and metadata}{28}{subsection.3.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{29}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Model Evaluation}{29}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Comparison of Results in NSTL with COLMAP and OpenCV}{29}{subsection.4.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Comparison of metrics for the scene from the NSTL dataset.}}{29}{table.caption.30}\protected@file@percent }
\newlabel{tab:3dgs_results_nstl}{{4.1}{29}{Comparison of metrics for the scene from the NSTL dataset}{table.caption.30}{}}
\citation{yang2023gs4d}
\citation{pumarola2021d}
\citation{li2022neural}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Comparison between ground truth and rendered image using COLMAP camera poses: The reconstruction shows clear textures in the central region, with minor blurring near the floor and image borders.}}{30}{figure.caption.31}\protected@file@percent }
\newlabel{fig:3dgs_COLMAP}{{4.1}{30}{Comparison between ground truth and rendered image using COLMAP camera poses: The reconstruction shows clear textures in the central region, with minor blurring near the floor and image borders}{figure.caption.31}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Ground Truth}}}{30}{subfigure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Rendered Image}}}{30}{subfigure.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Comparison between ground truth and rendered image using OpenCV calibration: The reconstruction maintains good texture fidelity in the central region, with stronger blurring near the floor and background.}}{30}{figure.caption.32}\protected@file@percent }
\newlabel{fig:3dgs_opencv}{{4.2}{30}{Comparison between ground truth and rendered image using OpenCV calibration: The reconstruction maintains good texture fidelity in the central region, with stronger blurring near the floor and background}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Ground Truth}}}{30}{subfigure.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Rendered Image}}}{30}{subfigure.2.2}\protected@file@percent }
\citation{yang2023gs4d}
\citation{yang2023gs4d}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}4D Gaussian Splatting}{31}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Yang et al.}{31}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Method Validation with D-NeRF and DyNeRF Datasets}{31}{section*.33}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Comparison of metrics for the "Mutant" scene from the D-NeRF dataset.}}{31}{table.caption.34}\protected@file@percent }
\newlabel{tab:mutant_metrics}{{4.2}{31}{Comparison of metrics for the "Mutant" scene from the D-NeRF dataset}{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Comparison of metrics for the "cut roasted beef" scene from the DyNeRF dataset.}}{31}{table.caption.37}\protected@file@percent }
\newlabel{tab:cut_roasted_beef_metrics}{{4.3}{31}{Comparison of metrics for the "cut roasted beef" scene from the DyNeRF dataset}{table.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Example reconstruction from a test camera: While central image elements are consistently reconstructed, the ceiling exhibits strong errors due to insufficient training data coverage.}}{32}{figure.caption.35}\protected@file@percent }
\newlabel{fig:DNeRFVergleich}{{4.3}{32}{Example reconstruction from a test camera: While central image elements are consistently reconstructed, the ceiling exhibits strong errors due to insufficient training data coverage}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Example reconstruction from a test camera: During high-motion frames (top), some blurring and incomplete reconstructions occur; in low-motion frames (bottom), the reconstruction quality is high across the scene.}}{33}{figure.caption.36}\protected@file@percent }
\newlabel{fig:DyNeRFVergleich}{{4.4}{33}{Example reconstruction from a test camera: During high-motion frames (top), some blurring and incomplete reconstructions occur; in low-motion frames (bottom), the reconstruction quality is high across the scene}{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results with the NSTL Dataset}{34}{section*.38}\protected@file@percent }
\newlabel{sec:results_nstl}{{4.2.1}{34}{Results with the NSTL Dataset}{section*.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Comparison of metrics for the NSTL "4DGS dataset" with varying numbers of Gaussians and sequence lengths.}}{34}{table.caption.39}\protected@file@percent }
\newlabel{tab:4dgs_metrics}{{4.4}{34}{Comparison of metrics for the NSTL "4DGS dataset" with varying numbers of Gaussians and sequence lengths}{table.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Comparison of ground truth (left) and rendered images (right) for the 4D Tennis dataset with 50 images.}}{35}{figure.caption.40}\protected@file@percent }
\newlabel{fig:DyNeRFVergleich}{{4.5}{35}{Comparison of ground truth (left) and rendered images (right) for the 4D Tennis dataset with 50 images}{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results of the Pruning Approach}{36}{section*.41}\protected@file@percent }
\newlabel{pruning_results}{{4.2.1}{36}{Results of the Pruning Approach}{section*.41}{}}
\@writefile{toc}{\contentsline {paragraph}{Analysis of Active Gaussians}{36}{section*.42}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Comparison of different pruning strategies: (a) proportion of active Gaussians over time, (b) distribution of $\Sigma _t$.}}{37}{figure.caption.44}\protected@file@percent }
\newlabel{fig:Pruning_overview}{{4.6}{37}{Comparison of different pruning strategies: (a) proportion of active Gaussians over time, (b) distribution of $\Sigma _t$}{figure.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Comparison of metrics for the NSTL "4DGS dataset" with varying Gaussian counts and memory usage.}}{37}{table.caption.43}\protected@file@percent }
\newlabel{tab:4dgs_metrics}{{4.5}{37}{Comparison of metrics for the NSTL "4DGS dataset" with varying Gaussian counts and memory usage}{table.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Comparison of ground truth (left) and rendered images (right) for camera C-1-M. While background details improve with pruning, foreground regions (hands) exhibit qualitative weaknesses.}}{38}{figure.caption.45}\protected@file@percent }
\newlabel{fig:4dgs_prune}{{4.7}{38}{Comparison of ground truth (left) and rendered images (right) for camera C-1-M. While background details improve with pruning, foreground regions (hands) exhibit qualitative weaknesses}{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Volley Dataset}{39}{subsection.4.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Comparison of metrics between the original and cleaned scene.}}{39}{table.caption.46}\protected@file@percent }
\newlabel{tab:4dgs_clean_metrics}{{4.6}{39}{Comparison of metrics between the original and cleaned scene}{table.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Training with Fixed Background}{39}{subsection.4.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces placeholder}}{40}{figure.caption.47}\protected@file@percent }
\newlabel{fig:4DTennis50img_GtvsRender}{{4.8}{40}{placeholder}{figure.caption.47}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Composition Pipeline Evaluation}{41}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Compositional Scene Generation}{41}{section.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces  \textbf  {Compositional scene generation.} Example of multi-object scenes composed from independently reconstructed Gaussian Splatting models. Each row shows one scene configuration with RGB, depth, bounding boxes, and segmentation overlays. All semantic annotations are generated automatically and remain spatially consistent across views. }}{42}{figure.caption.48}\protected@file@percent }
\newlabel{fig:composition}{{4.9}{42}{\textbf {Compositional scene generation.} Example of multi-object scenes composed from independently reconstructed Gaussian Splatting models. Each row shows one scene configuration with RGB, depth, bounding boxes, and segmentation overlays. All semantic annotations are generated automatically and remain spatially consistent across views}{figure.caption.48}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Multi-view Occlusion Handling}{42}{section.4.5}\protected@file@percent }
\citation{schoenberger2016sfm}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces  \textbf  {Occlusion handling across views.} Segmentation overlays from front and side viewpoints of two interacting subjects. Even under strong mutual occlusion, instance masks remain consistent and well aligned with visible contours. }}{43}{figure.caption.49}\protected@file@percent }
\newlabel{fig:occlusion}{{4.10}{43}{\textbf {Occlusion handling across views.} Segmentation overlays from front and side viewpoints of two interacting subjects. Even under strong mutual occlusion, instance masks remain consistent and well aligned with visible contours}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Scene Generalization}{43}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Temporal Consistency in Dynamic Scenes}{43}{section.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces  \textbf  {Scene generalization and recontextualization.} Dynamic subjects composited into a novel virtual environment reconstructed with COLMAP. The system maintains consistent geometry and segmentation alignment across domains, demonstrating flexible recontextualization of pre-trained Gaussian models. }}{44}{figure.caption.50}\protected@file@percent }
\newlabel{fig:generalization}{{4.11}{44}{\textbf {Scene generalization and recontextualization.} Dynamic subjects composited into a novel virtual environment reconstructed with COLMAP. The system maintains consistent geometry and segmentation alignment across domains, demonstrating flexible recontextualization of pre-trained Gaussian models}{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces  \textbf  {Dynamic scene generation.} Representative time steps of a moving subject with RGB and segmentation overlays. The consistent motion and geometry across frames demonstrate temporally stable 4D rendering. }}{44}{figure.caption.51}\protected@file@percent }
\newlabel{fig:temporal}{{4.12}{44}{\textbf {Dynamic scene generation.} Representative time steps of a moving subject with RGB and segmentation overlays. The consistent motion and geometry across frames demonstrate temporally stable 4D rendering}{figure.caption.51}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Qualitative Pose Estimation}{44}{section.4.8}\protected@file@percent }
\citation{Detectron22020}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces  \textbf  {Qualitative analysis of pose and keypoint propagation.} Four frames of a dynamic subject showing estimated object poses (top) and propagated keypoints (bottom). The coordinate systems evolve consistently over time, while keypoint trajectories remain coherent despite local misalignments. }}{45}{figure.caption.52}\protected@file@percent }
\newlabel{fig:keypoints}{{4.13}{45}{\textbf {Qualitative analysis of pose and keypoint propagation.} Four frames of a dynamic subject showing estimated object poses (top) and propagated keypoints (bottom). The coordinate systems evolve consistently over time, while keypoint trajectories remain coherent despite local misalignments}{figure.caption.52}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Keypoint Propagation}{45}{section.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Summary and Outlook}{46}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Summary of Key Findings}{46}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Limitations}{47}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Future Developments and Research Perspectives}{47}{section.5.3}\protected@file@percent }
\bibstyle{plain}
\bibcite{barron2021mip}{1}
\bibcite{barron2023zipnerf}{2}
\bibcite{MaskPropagation2019}{3}
\bibcite{bertasius2019maskprop}{4}
\bibcite{Bertasius2020}{5}
\bibcite{Denninger2019}{6}
\bibcite{denninger2019blenderproc}{7}
\bibcite{MobileRobotsGS2024}{8}
\bibcite{Dwibedi2017}{9}
\@writefile{toc}{\contentsline {chapter}{References}{49}{section.5.3}\protected@file@percent }
\newlabel{Sec:1}{{5.3}{49}{References}{section*.53}{}}
\bibcite{dwibedi2017cutpaste}{10}
\bibcite{everingham2010pascal}{11}
\bibcite{fridovich2023k}{12}
\bibcite{godard2017unsupervised}{13}
\bibcite{MonoDepthLimitations2018}{14}
\bibcite{Godard2019}{15}
\bibcite{He2017MaskRCNN}{16}
\bibcite{he2017mask}{17}
\bibcite{kerbl3Dgaussians}{18}
\bibcite{Kirillov2023}{19}
\bibcite{krizhevsky2012imagenet}{20}
\bibcite{lecun2015deep}{21}
\bibcite{Lee2018}{22}
\bibcite{Li2023MattingSurvey}{23}
\bibcite{li2022neural}{24}
\bibcite{lispacetimegaussianfeaturesplattingrealtime2024}{25}
\bibcite{lin2014microsoft}{26}
\bibcite{Liu2018}{27}
\bibcite{luiten2024dynamic}{28}
\bibcite{markley2007averaging}{29}
\bibcite{mildenhall2021nerf}{30}
\bibcite{Niu2021}{31}
\bibcite{pumarola2021d}{32}
\bibcite{ravi2024sam2}{33}
\bibcite{schoenberger2016sfm}{34}
\bibcite{schoenberger2016mvs}{35}
\bibcite{stearnsmarbels}{36}
\bibcite{SyntheticDrone2023}{37}
\bibcite{sun2017revisiting}{38}
\bibcite{Tobin2017}{39}
\bibcite{tobin2017domain}{40}
\bibcite{CutAndSplat2024}{41}
\bibcite{wu20244d}{42}
\bibcite{Detectron22020}{43}
\bibcite{yang2023gs4d}{44}
\bibcite{yang2024deformable}{45}
\bibcite{gaussian_grouping}{46}
\bibcite{InpaintingLimitations2019}{47}
\bibcite{yuan20251000fps4dgaussian}{48}
\bibcite{zanjani2025gaussiansplattingeffectivedata}{49}
\bibcite{zanjani2025gaussian}{50}
\bibcite{SurgicalGS2023}{51}
\bibcite{zollhoefer2018state}{52}
\bibcite{zwicker2001ewa}{53}
\csname bt@set@cnt\endcsname{53}
\gdef \@abspage@last{60}
