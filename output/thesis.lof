\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Core principle of NeRF: A model is optimized from input images and can subsequently generate novel views of the scene (after \cite {mildenhall2021nerf}).}}{3}{figure.caption.3}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces NeRF pipeline: The MLP maps 5D inputs to color and density, which are composed into an image via volumetric rendering (after \cite {mildenhall2021nerf}).}}{4}{figure.caption.4}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Optimization and rendering pipeline of Gaussian Splatting: The process begins with a sparse structure-from-motion (SfM) point cloud, which serves as the basis for creating an initial set of 3D Gaussian functions. These Gaussians are refined through iterative optimization, with their density adaptively controlled to ensure accurate scene representation. A fast tile-based rasterizer enables competitive rendering times compared to modern radiance field methods (figure from \cite {kerbl3Dgaussians}).}}{5}{figure.caption.5}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Adaptive density control: The top row shows under-reconstruction, where small geometries (black outlines) are supplemented by cloning a Gaussian. The bottom row shows over-reconstruction, where a large Gaussian is split into two smaller ones (after \cite {kerbl3Dgaussians}).}}{8}{figure.caption.9}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Schematic illustration of the local rotation similarity loss in D3DGS~\cite {luiten2024dynamic}.}}{13}{figure.caption.14}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf {Overview of the proposed composition pipeline.} \textbf {Pipeline A} converts multi-view captures into per-object 3DGS and D3DGS models via calibration, mask generation, and model training. \textbf {Pipeline B} composes exported models into multi-object scenes, producing synchronized RGB, depth, segmentation, and occlusion annotations. }}{17}{figure.caption.20}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces \textbf {Compositional scene generation.} Example of multi-object scenes composed from independently reconstructed Gaussian Splatting models. Each row shows one scene configuration with RGB, depth, bounding boxes, and segmentation overlays. All semantic annotations are generated automatically and remain spatially consistent across views. }}{23}{figure.caption.23}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces \textbf {Occlusion handling across views.} Segmentation overlays from front and side viewpoints of two interacting subjects. Even under strong mutual occlusion, instance masks remain consistent and well aligned with visible contours. }}{24}{figure.caption.24}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces \textbf {Scene generalization and recontextualization.} Dynamic subjects composited into a novel virtual environment reconstructed with COLMAP. The system maintains consistent geometry and segmentation alignment across domains, demonstrating flexible recontextualization of pre-trained Gaussian models. }}{24}{figure.caption.25}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces \textbf {Dynamic scene generation.} Representative time steps of a moving subject with RGB and segmentation overlays. The consistent motion and geometry across frames demonstrate temporally stable 4D rendering. }}{25}{figure.caption.26}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces \textbf {Qualitative analysis of pose and keypoint propagation.} Four frames of a dynamic subject showing estimated object poses (top) and propagated keypoints (bottom). The coordinate systems evolve consistently over time, while keypoint trajectories remain coherent despite local misalignments. }}{26}{figure.caption.27}%
\addvspace {10\p@ }
