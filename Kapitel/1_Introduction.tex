\section{Introduction}

\subsection{Motivation}
Artificial intelligence has made remarkable progress in understanding and generating visual content \cite{lecun2015deep, krizhevsky2012imagenet}. Modern computer vision systems can recognize objects, estimate depth, and reconstruct three-dimensional scenes from ordinary images \cite{he2017mask, zollhoefer2018state}. At the core of these capabilities lies the data used to train and evaluate such models \cite{sun2017revisiting}. Most deep learning approaches rely on large collections of labeled data, yet creating high-quality datasets with accurate geometry and temporal consistency remains one of the most expensive and time-consuming aspects of research and development \cite{everingham2010pascal, lin2014microsoft}.

In recent years, visual scene representation has shifted from traditional image-based techniques toward neural and hybrid three-dimensional representations. Neural scene representation aims to describe a scene not as a set of disconnected pixels but as a continuous spatial structure that can be observed from any viewpoint \cite{mildenhall2021nerf}. Methods such as Neural Radiance Fields (NeRFs) have shown that continuous three-dimensional models can be reconstructed from ordinary photographs, although these methods often involve long optimization times and complex rendering processes \cite{barron2021mip}.

Gaussian Splatting has recently emerged as a faster and more transparent alternative \cite{kerbl3Dgaussians}. Instead of relying on deep implicit functions, it models a scene as a collection of Gaussian primitives representing color, position, and opacity in space. This approach allows real-time rendering while maintaining a high level of detail and realism \cite{fridovich2023k}. Because Gaussian primitives can be easily manipulated and recombined, they offer new possibilities for modular scene generation and synthetic data creation \cite{CutAndSplat2024}.

The generation of synthetic data is increasingly important for artificial intelligence \cite{tobin2017domain, denninger2019blenderproc}. Real-world datasets are often limited by annotation errors, missing labels, and insufficient diversity \cite{dwibedi2017cutpaste}. Synthetic datasets can overcome these limitations by providing unlimited, precisely labeled, and perfectly synchronized examples \cite{zanjani2025gaussian}. They are particularly valuable in scenarios where ground-truth information such as depth, motion, or segmentation is difficult to obtain manually \cite{godard2017unsupervised, bertasius2019maskprop}.

For training robust AI models, both the quality and diversity of data are critical. A system that can automatically generate realistic, correctly annotated images from structured three-dimensional representations thus offers significant advantages for both academic research and industrial applications.

Despite this potential, most existing methods focus on reconstructing individual static or dynamic scenes \cite{luiten2024dynamic,yang2024deformable}, without providing tools for composing multiple objects or actors into coherent multi-view environments with synchronized annotations. Many practical applications—such as robotics, autonomous systems, and simulation—require dynamic scenes that include interacting elements while maintaining accurate geometry and temporal alignment \cite{CutAndSplat2024}.

\subsection{Structure of the Work}
This thesis addresses the aforementioned challenges by developing a modular pipeline based on static and dynamic Gaussian Splatting. The proposed system enables the reconstruction, composition, and rendering of complex multi-object scenes in calibrated camera setups. It produces synchronized outputs including color images, depth maps, and instance-level segmentation. Additionally, the framework supports annotations such as keypoints, occlusion scores, and estimated poses, providing a comprehensive solution for generating structured and reproducible synthetic datasets.

The approach emphasizes compositional reliability rather than algorithmic novelty. By integrating existing Gaussian Splatting methods into a unified workflow, it demonstrates how realistic and temporally consistent synthetic data can be generated with minimal manual effort. In this way, the system bridges the gap between recent advances in neural scene representation and the practical need for scalable dataset generation.

The remainder of this thesis is organized as follows. Chapter~\ref{chap:Fundamentals} introduces the theoretical foundations of Gaussian Splatting. Chapter~\ref{chap:State_of_the_Art} reviews existing methods and positions the proposed system within the broader landscape of Gaussian-based representations and related work in dynamic scene modeling, including a detailed discussion of the two most prominent approaches. Chapter~\ref{chap:Methodology} describes the modifications and training improvements applied, followed by a step-by-step presentation of the completed pipeline. Chapter~\ref{chap:Results} presents reconstruction results for both static and dynamic Gaussian Splatting and provides a qualitative evaluation of the newly constructed pipeline. Finally, Chapter~\ref{chap:Conclusion} summarizes the key findings, discusses limitations, and outlines potential directions for future research.
