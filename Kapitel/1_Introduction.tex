\chapter{Introduction}

% Recent advances in neural scene representations have demonstrated that \emph{Gaussian Splatting (GS)} \cite{kerbl3Dgaussians} provides an efficient and photorealistic alternative to Neural Radiance Fields (NeRFs) \cite{mildenhall2021nerf} for multi-view reconstruction. Static 3D Gaussian Splatting (3DGS) methods enable compact and high-fidelity rendering of object-level models, while dynamic extensions such as \emph{Dynamic 3D Gaussians (D3DGS)} \cite{luiten2024dynamic} and several 4D GS variants achieve temporally coherent reconstructions of moving agents. These techniques have established a practical foundation for geometry-aware, image-based rendering and synthetic data generation.

% While these methods are effective for reconstructing individual static or dynamic scenes, less attention has been paid to the problem of \emph{composing} multiple Gaussian-based models into coherent, multi-object, and temporally consistent scenes. Such compositional modeling is highly relevant for dataset generation, simulation, and augmentation, yet it introduces several practical challenges: (i) spatial alignment of exported models across captures, (ii) deterministic occlusion handling in multi-view configurations, and (iii) the generation of temporally consistent multi-modal annotations. Existing approaches such as \emph{Cut-and-Splat} \cite{CutAndSplat2024} explore object-level manipulation but rely on heuristic background placement and do not offer a principled framework for integrating static and dynamic Gaussian models.

% In this paper, we present a modular\emph{composition pipeline} that integrates existing 3DGS and D3DGS models into calibrated multi-view scenes. The system enables the controlled assembly and rendering of multi-object and dynamic scenes while automatically generating synchronized RGB, depth, and instance-level annotations. It includes both \emph{visible} and \emph{complete} instance masks, per-instance bounding boxes, occlusion scores, and candidate 6D placement transforms. In addition, 2D keypoints obtained from a Detectron2 detector \cite{Detectron22020} are temporally propagated to provide motion-consistent supervisory signals for downstream tasks. 
% %Dynamic object reconstructions are supported by \emph{SAM2}-based mask generation \cite{ravi2024sam2}, which ensures temporally stable supervision across all views.

% Because the generated data is fully synthetic, no external ground-truth annotations are available for quantitative comparison. Consequently, our evaluation focuses on qualitative analysis of spatial and temporal consistency across views and over time. We emphasize visual coherence as the primary indicators of system reliability.

% The proposed framework is demonstrated in a controlled, hardware-synchronized 56-camera rig, showing that it produces temporally stable renderings and geometrically consistent multi-modal annotations. By emphasizing compositional reproducibility rather than algorithmic modification, this work bridges the gap between reconstruction-focused Gaussian Splatting methods and practical, multi-object dataset generation workflows.

% \noindent\textbf{Contributions.} The main contributions of this work are summarized as follows:

% \noindent\begin{enumerate}
%     \item A composition pipeline that integrates existing static (3DGS) and dynamic (D3DGS) Gaussian Splatting models into calibrated multi-view scenes, producing synchronized RGB, depth, and instance-level annotations.
%     \item An integrated annotation suite that provides visible and complete instance masks, occlusion scoring, per-instance bounding boxes, candidate 6D placement transforms, and temporally propagated 2D keypoints.
%     \item A qualitative assessment of composition quality and temporal consistency in controlled capture settings, highlighting internal coherence and reproducibility.
% \end{enumerate}


Artificial intelligence has made remarkable progress in understanding and generating visual content \cite{lecun2015deep, krizhevsky2012imagenet}. Modern computer vision systems can recognize objects, estimate depth, and reconstruct three-dimensional scenes from ordinary images \cite{he2017mask, zollhoefer2018state}. Behind these capabilities lies an essential component: the data used to train and evaluate such models \cite{sun2017revisiting}. Most deep learning approaches rely on large collections of labeled data, yet creating high-quality datasets with accurate geometry and temporal consistency remains one of the most expensive and time-consuming parts of research and development \cite{everingham2010pascal, lin2014microsoft}.

In recent years, the representation of visual scenes has shifted from traditional image-based techniques toward neural and hybrid three-dimensional representations. Neural scene representation aims to describe a scene not as a set of disconnected pixels but as a continuous spatial structure that can be viewed from any angle \cite{mildenhall2021nerf}. Methods such as Neural Radiance Fields (NeRFs) have shown that it is possible to reconstruct continuous three-dimensional models from ordinary photographs, although they often require long optimization times and complex rendering processes \cite{barron2021mip}.

Gaussian Splatting has recently emerged as a faster and more transparent alternative \cite{kerbl3Dgaussians}. Instead of relying on deep implicit functions, it models a scene as a collection of Gaussian primitives that represent color, position, and opacity in space. This structure allows real-time rendering while maintaining a high level of detail and realism \cite{fridovich2023k}. Because Gaussian primitives can be easily manipulated and recombined, this approach opens new possibilities for modular scene generation and synthetic data creation \cite{CutAndSplat2024}.

The ability to generate synthetic data is becoming increasingly important for artificial intelligence \cite{tobin2017domain, denninger2019blenderproc}. Real-world datasets are often limited by annotation errors, missing labels, and a lack of variety \cite{dwibedi2017cutpaste}. Synthetic datasets can overcome these problems by providing unlimited, precisely labeled, and perfectly synchronized examples \cite{zanjani2025gaussian}. They are especially valuable in areas where ground-truth information such as depth, motion, or segmentation is difficult to obtain through manual annotation \cite{godard2017unsupervised, bertasius2019maskprop}.
For training robust AI models, it is not only the amount of data that matters but also the quality, consistency, and diversity of the examples. A system that can automatically generate realistic and correctly annotated images from structured three-dimensional representations therefore offers a significant advantage for both academic research and industrial applications.

Despite this potential, most existing methods focus on reconstructing individual static or dynamic scenes \cite{luiten2024dynamic,yang2024deformable}. They do not provide tools for composing multiple objects or actors into coherent multi-view environments with synchronized annotations. In many practical scenarios such as robotics, autonomous systems, or simulation, it is essential to create dynamic scenes that include several interacting elements while maintaining accurate geometry and temporal alignment \cite{CutAndSplat2024}. 

This thesis addresses these challenges by developing a modular pipeline based on static and dynamic Gaussian Splatting. The proposed system allows the reconstruction, composition, and rendering of complex multi-object scenes in calibrated camera setups. It produces synchronized outputs that include color images, depth maps, and instance-level segmentation. The framework also supports additional annotations such as keypoints, occlusion scores, and estimated poses, which makes it a complete solution for generating structured and reproducible synthetic datasets.

The approach emphasizes compositional reliability rather than algorithmic novelty. By integrating existing Gaussian Splatting methods into a unified workflow, it demonstrates how realistic and temporally consistent synthetic data can be created with minimal manual effort. The system thus connects the recent progress in neural scene representation with the practical need for scalable dataset generation.

The remainder of this thesis presents the theoretical background, the design of the proposed pipeline, and a qualitative evaluation of its results. The findings show that Gaussian-based representations can serve not only as tools for rendering but also as a foundation for controllable four-dimensional data generation and future research in dynamic scene understanding.